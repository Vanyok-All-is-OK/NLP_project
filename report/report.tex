\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2.5cm]{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{multicol}
\usepackage[hidelinks]{hyperref}
\usepackage{tikz}
\usepackage{wasysym}
\usepackage{amsmath}
\usepackage[most]{tcolorbox}
\usepackage{parskip}
\graphicspath{{images/}}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}


\usepackage{fontspec}
\setmainfont{Helvetica}
% \setmainfont{CMU Bright}
% \setmainfont{CMU Serif}

\hypersetup{
    colorlinks=true,
    linkcolor=cyan, % blue
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={HW1 Dmitry Uspenskiy},
    pdfpagemode=FullScreen,
}

\newcommand{\imgh}[3]
{
\begin{figure}[h]
\center{\includegraphics[width=#1]{#2}}
\caption{#3}
\label{ris:#2}
\end{figure}
}

\newcommand{\condition}[1]
{
\begin{tcolorbox}[enhanced jigsaw,
    sharp corners,
    boxrule=0.5pt, 
    colback=white!30!white,   
    borderline={0.5pt}{-2pt}{black,solid} % 0.5pt linewith, -2pt outside, black solid linestyle
]
#1
\end{tcolorbox}
}

% \setcounter{section}{-1} %Нумерация с 0
\hyphenpenalty=10000

\pagestyle{fancyplain}
\headheight 35pt
\rhead{\textbf{Выполнили:} Успенский Д. А. \\ Беляев И. А. \\ Карбаев С. А. \\ \textbf{Группа:} 208}
\chead{\textbf{\large КТ 1} \\ [3ex] }
\lhead{ФКН ВШЭ \\ Автоматическая Обработка Текста \\ Осенний семестр 2023 \\ } 
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 3em

\begin{document}
\tableofcontents
\newpage

\section{Описание предметной области и постановка задачи}
В качестве проекта мы выбрали \href{https://semantic-textual-relatedness.github.io/}{\textit{Task 1: Semantic Textual Relatedness (STR)}}, где предлагается решить задачу по автоматическому определению степени семантической связи между парами предложений на различных языках.

\subsection{Введение в предметную область}
\textbf{Semantic Textual Relatedness} (семантическая текстовая родственность) показывает степень, в которой две единицы языка, такие как слова, фразы или предложения, близки по своему значению. Это мера того, насколько семантически схожи или родственны две лингвистические единицы друг другу. Задача измерения этой связи возникает естественным образом при работе с текстовыми данными.

Применения меры семантического сходства:
\begin{itemize}
    \item \textbf{Лингвистические}
    \begin{itemize}
        \item изучение отношения между различными языковыми единицами
        \item измерение тесктовой связности
        \item сравнение стиля текстов на различных языках
    \end{itemize}

    \item \textbf{NLP}
    \begin{itemize}
        \item поиск информации в тексте
        \item обобщение документов
        \item оценка методологий: инструмент сравнения с groud truth текстами
        \item создание датасетов: наборы данных семантической текстовой связанности, таких как STR-2022 (Mohamed Abdalla et. al. , 2023)
    \end{itemize}
\end{itemize}

\subsection{Постановка задачи}
На соревновании мы выбрали трек \textit{A: Supervised} на английском языке. Необходимо обучить модель, которая предсказываеи STR двух предложений. Доступны тренировчный и тестовый датасеты на английском, \href{https://codalab.lisn.upsaclay.fr/competitions/15715#results}{Leaderboard}.

\newpage

\section{Обзор литературы}

\begin{enumerate}
	\item  \href{https://www.researchgate.net/publication/272088094_A_SEMANTIC_SIMILARITY_MEASURE_BETWEEN_SENTENCES}{A Semantic Similarity Measure Between Sentences} \\
	{\footnotesize Manh Hung Nguyen, Dinh Que Tran, 2014}
	
	В статье представлена математическая модель для оценки семантического сходства с различными онтологиями, то есть математическое представление семантического расстояния между понятиями в онтологии. Они создают множество слов из каждого предложения, затем смотрят для каждого слова, слово в другом предложении с максимальной мерой схожести. В итоге они получают два семантических вектора 1-го предложения по отношению ко 2-му и наоборот. Решением этой задачи будет вычисление функции $f_{noss}$ от этих двух векторов, где $f_{noss}$ это функция схожести двух неупорядоченных векторов.
	
	\item \href{https://www.researchgate.net/publication/322512205_Mined_semantic_analysis_A_new_concept_space_model_for_semantic_representation_of_textual_data}{Mined semantic analysis: A new concept space model for semantic representation of textual data} \\
	{\footnotesize Walid Shalaby, et. al., 2017}
	
	Основная идея этой модели - получение большего охвата вокруг слова. В этом отношении другие модели (ESA, SSA и NASARI) используют явные понятия из энциклопедий, а модель Mined Semantic Analysis рассматривает также граф ссылок Википедии "Смотрите также". Благодаря этому получилось улучшить результат относительно прошлых решений.
	
	\item \href{https://www.researchgate.net/publication/349481951_Evolution_of_Semantic_Similarity-A_Survey}{Evolution of Semantic Similarity—A Survey} \\
	{\footnotesize Dhivya Chandrasekaran and Vijay Mago, 2021}
	
	В этой статье обсуждается проблема оценки семантического сходства между текстами, а также различные подходы которые появлялись в разные моменты времени. В работе прослеживается эволюция методов семантического сходства, классифицируя их в зависимости от лежащих в основе принципов: knowledge-based, corpus-based, глубокие нейронные сети и гибридные методы.
	Опишем их всех кратко.
	\begin{itemize}
		\item Сходство knowledge-based основан на определении степени сходства между словами с использованием информации, полученной из семантических сетей. WordNet - самая популярная семантическая сеть в области измерения \\ основанного на знаниях сходства между словами. (Семантическая сеть — информационная модель предметной области, имеет вид ориентированного графа. Вершины графа соответствуют объектам предметной области, а дуги (рёбра) задают отношения между ними).

		\item Сходство corpus-based представляет собой простое сравнение символов в n-граммах.

		\item Методы, основанные на глубоких нейронных сетях, такие как модели на основе трансформеров, были предложены совсем недавно для оценки \\ семантического сходства. Они используют нейронные сети для определения схожести двух предложений. 
        \newpage

		\item Гибридные методы являются комбинацией различных методов, соединяя которые, стремятся улучшить результат чистых моделей.
	\end{itemize}
	(также можно почитать \href{https://habr.com/ru/companies/skillfactory/articles/566414/}{эту статью на Habr'e}, в ней короче описано то же самое, но с большими примерами кода)
	
	В нашем исследовании мы приняли решение использовать BERT модель в качестве основы, поскольку имеющиеся результаты, основанные на моделях, использующих BERT, продемонстрировали более высокую эффективность по сравнению с альтернативными моделями, представленными выше. В соответствии с данными, представленными в таблице статьи, точность систем, основанных на BERT, достигает 90\%.
\end{enumerate}



\newpage

\section{Описание датасета}
\subsection{Структура датасета}
Каждый объект в датасете представляет собой пару предложений и оценку, отражающую степень семантической текстовой связи между двумя предложениями. Баллы могут варьироваться от 0 (максимально не связаны) до 1 (максимально связаны). Эти оценки были определены путем ручного аннотирования. В частности, использовался метод \textit{comparative annotation}, чтобы избежать известных ограничений традиционных методов оценки сходства.

Датасет имеет следующую структуру:
\begin{itemize}
    \item \textbf{PairID} - уникальный id пары предложений
    \item \textbf{Text} - тексты, записанные подряд через ' \textbackslash n'
    \item \textbf{Score} - оценка семантической близости предложений
\end{itemize}

Тренировочный датасет - 5500 объектов; тестовый датасет - 250.

\begin{table}[H]
\center
\tiny
\begin{tabular}{llllr}
\hline
{} &          PairID &                                             Text\_1 &                                             Text\_2 &  Score \\
\hline
0 &  ENG-train-0000 &               It that happens, just pull the plug. &          if that ever happens, just pull the plug. &    1.0 \\
1 &  ENG-train-0001 &                 A black dog running through water. &         A black dog is running through some water. &    1.0 \\
2 &  ENG-train-0002 &       I've been searchingthe entire abbey for you. &            I'm looking for you all over the abbey. &    1.0 \\
3 &  ENG-train-0003 &  If he is good looking and has a good personali... &  If he's good looking, and a good personality, ... &    1.0 \\
4 &  ENG-train-0004 &  She does not hate you, she is just annoyed wit... &         She doesn't hate you, she is just annoyed. &    1.0 \\
\hline
\end{tabular}
\caption{Пример выборки (тексты были разбиты на пары)}
\end{table}

\subsection{Измерение качества}
Официальной метрикой оценки для этого задания является коэффициент ранговой корреляции Спирмена.
$$
r_s = \rho_{R(X), R(Y)} = \frac{cov(R(X), R(Y))}{\sigma_{(R(X))} \sigma_{(R(Y))}}
$$
\begin{itemize}
    \item $\rho$ обозначает коэффициент корреляции Пирсона, но применяется к ранговым переменным
    \item $cov(R(X), R(Y))$ является ковариацией ранговых переменных
    \item $\sigma_{(R(X))}$ и $\sigma_{(R(Y))}$ являются стандартными отклонениями ранговых переменных
\end{itemize}
Коэффициент отражает, насколько хорошо предсказанные системой скоры согласуются с суждениями человека.

\newpage

\section{Эксплоративный анализ}
Для предварительного анализа и имплементации бейслайна пары предложений в датасете были лемматизированны и очищены от стоп-слов. Сам датасет был разбит на тестовую и валидационную выборки в отношении $0.8 / 0.2$.

Мы удостоверились, что датасет чист от выбросов и пробелов.

\subsection{Распределение Score}
Посмотрим на распределение скоров по частоте и в зависимости от модуля разности длин пары предложений.
\imgh{17cm}{score_distribution.png}{Распределение Score}
Для


Распределение по целевой функции похоже на нормальное, есть концентрация плотности в значении 0.5. 

Также видим, что скор обратно пропорционален разности длин текстов, что соответствует интуиции: длинные предложения обычно имеют смысл отличные от коротких.
\newpage

\subsection{Схожести текстов}
Дополнительно взглянем на близости текстов в датасете. В качестве меры близости будем использовать WER на лемматизированных предложениях:
$$ WER = \frac{S + D + I}{N} =  \frac{S + D + I}{S + D + C}$$
где:
\begin{itemize}
    \item \textit{S} — количество замен,
    \item \textit{D} — количество удалений,
    \item \textit{I} — количество вставок,
    \item \textit{C} — количество правильных слов,
    \item \textit{N} — количество слов в ссылке $(N = S + D + C)$
\end{itemize}

Рассмотрим распределение WER по частоте и в зависимости от скора. 

\imgh{17cm}{wer_distribution.png}{Распределение WER}

Мы видим, что на лемматизированных текстах WER в среднем равен 1. Это означает, что пары текстов различны морфологически в основном (то есть наборами, представляющих их слов).

Ожидаемо, чем ниже WER - тем лучше Score.
\newpage

\section{Бейслайн решение}

В качестве бейслайна авторы предложили посчитать долю общих слов в предложениях.

Оба предложения токенизируются по словам и знакам препинания с помощью регулярного выражения, и составляются множества токенов каждого из этих предложений. После этого предсказанная степень сходства вычисляется как мощность пересечения этих множеств, делённая на суммарную мощность обоих множеств.
На лемматированных текстах такой подход даёт коррелацию Пирсона 0.58.

\href{https://github.com/semantic-textual-relatedness/Semantic_Relatedness_SemEval2024/blob/main/STR_Baseline.ipynb}{Collab} с авторским решением.

В качестве дополнительного подхода мы обучили TF\_IDF модель на тестовой выборке. Над векторами был обучен RandomForestClassifier. Такой метод дал следующие скоры:
\begin{itemize}
    \item MSE: 0.04
    \item Spearman correlation: 0.435
    \item Pearson correlation: 0.433
\end{itemize}

\section{Описание выбранной базовой архитектуры}
В рамках данной части проекта ставилась задача выдвинуть архитектуру модели, которая превзойдёт базовое решение организаторов по целевой метрике качества.

Для достижения этой цели использовался следующий метод.
\begin{itemize}
    \item Для каждого предложения в выборке вычислялся его BERT-эмбеддинг как \\ усреднённые по всем токенам в предложении выходные эмбеддинги последнего слоя модели bert-base-uncased.

    \item Семантическое сходство пары предложений оценивалось как косинусное сходство BERT-эмбеддингов этих предложений (косинус угла между эмбеддингами).
\end{itemize}

Использовался предобученный \href{https://huggingface.co/bert-base-uncased}{bert-base-uncased}.

Данный метод был выбран из-за простоты реализации, а также потому, что он развивает концепцию анализа слов предложений из базового решения организаторов на более сложные признаки предложений, такие как BERT-эмбеддинги. Таким образом, можно чётко отследить вклад в качество решения, который даёт переход от анализа совместных слов к анализу BERT-эмбеддигов предложений.

Также важно отметить, что данное решение фактически относится к unsupervised-подходу, т.к. не использует целевую переменную при обучении. С одной стороны, это является недостатком из-за потери информации от целевой переменной. Однако такой подход избавляет от проблемы переобучения сложных моделей под выборку, часто возникающей в задачах с маленькой обучающей выборкой (в данном случае её размер составляет 5500 пар предложений).

В будущей работе можно попробовать улучшить качество решения путём перехода к supervised-подходу с помощью методов регуляризации.

\newpage


\section{Описание полученных результатов}
Чтобы оценить предложенное решение, в тестирующую систему соревнования нашей командой было отправлено как наше решение, так и базовое решение организаторов для сравнения значений функционала качества.

Результаты получились следующими. Базовое решение организаторов набрало score 0.63, а решение, предложенное нами - 0.67 (значения округлены до сотых). Таким образом, наше решение превзошло базовое решение организаторов по значению целевого функционалу качества. \href{https://codalab.lisn.upsaclay.fr/my/competition/submission/588203/detailed_results/}{Ссылка} на посылку в контест.

\href{https://github.com/wwwwwert/NLP_project}{Ссылка} на репозиторий проекта.

\section{Направления дальнейшей работы}
В качестве направления дальнейшей работы предлагается дообучить Transformer-based модель на Score. Дополнительно стоит рассмотреть поведение модели при инференсе на разных ground-truth скорах - а именно, где она чаще ошибается.

Отдельным направлением хочется выделить сбор и разметку данных. Тренировочная выборка в 5500 элементов мала. Также хочется посмотреть как соотносятся предсказания реальных людей с предложенной в датасете разметкой. Эту проблему можно решить с помощью краудсорсинга, например Toloka.
\newpage


\section{Источники}
\begin{itemize}
    \item Nedjma Ousidhoum et. al. SemEval 2024 Task 1: Semantic Textual Relatedness (STR)
   \href{https://semantic-textual-relatedness.github.io/}{Source}

    \item Mohamed Abdalla et. al. , 2023 "What Makes Sentences Semantically Related?
    A Textual Relatedness Dataset and Empirical Study"

    \item Manh Hung Nguyen, Dinh Que Tran, 2014 "A Semantic Similarity Measure Between Sentences"

    \item Walid Shalaby, et. al., 2017 "Mined semantic analysis: A new concept space model for semantic representation of textual data"

    \item Dhivya Chandrasekaran and Vijay Mago, 2021 "Evolution of Semantic Similarity—A Survey"
\end{itemize}
\end{document}
